/* eslint-disable @typescript-eslint/no-unsafe-assignment */
/* eslint-disable @typescript-eslint/no-unsafe-member-access */
/* eslint-disable @typescript-eslint/no-unsafe-call */
/* eslint-disable @typescript-eslint/no-unsafe-return */
/* eslint-disable @typescript-eslint/no-unnecessary-type-assertion */
import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import {
  END,
  MessagesAnnotation,
  START,
  StateGraph,
} from '@langchain/langgraph';
import { MemorySaver } from '@langchain/langgraph-checkpoint';
import { ChatOpenAI } from '@langchain/openai';
import { Injectable } from '@nestjs/common';
import crypto from 'crypto';

@Injectable()
export class AppService {
  private readonly model: ChatOpenAI;
  private readonly checkpointer: MemorySaver;

  constructor() {
    this.model = new ChatOpenAI({
      modelName: 'gpt-4o',
    });

    this.checkpointer = new MemorySaver();
  }

  test2(prompt: string) {
    // SHA-256 í•´ì‹œ ìƒì„± (64ìž ê³ ì •)
    const hash = crypto.createHash('sha256').update(prompt).digest('hex');
    console.log('í•´ì‹œê°’:', hash);
    return hash;
  }

  async test(prompt: string, userId: number, isErr: boolean = false) {
    // âœ… ìƒíƒœ ì¶”ì ì„ ìœ„í•œ Annotation

    const openAiFunc = async (state: any) => {
      console.log('openAiFunc');
      const chatPrompt = ChatPromptTemplate.fromMessages([
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'human', content: '{prompt}' },
      ]);

      const aiResponse = await chatPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser())
        .invoke({ prompt });

      return { messages: aiResponse };
    };

    // ë³‘ë ¬ë¡œ ì‹¤í–‰ë  ë…¸ë“œë“¤ - ê°ê° ë…ë¦½ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°’ë§Œ ì¶”ê°€
    const node1 = async (state: any) => {
      console.log('Node 1 executed, current messages:');
      const chatPrompt = ChatPromptTemplate.fromMessages([
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'human', content: '{prompt}' },
      ]);

      const aiResponse = await chatPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser())
        .invoke({ prompt: '3 ê³±í•˜ê¸° 1ì€?' });

      return { messages: aiResponse };
    };

    const node2 = async (state: any) => {
      console.log('Node 2 executed, current messages:');
      const chatPrompt = ChatPromptTemplate.fromMessages([
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'human', content: '{prompt}' },
      ]);

      const aiResponse = await chatPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser())
        .invoke({ prompt: '3 ê³±í•˜ê¸° 2ëŠ”?' });

      return { messages: aiResponse };
    };

    // const workflow1 = (new StateGraph(MessagesAnnotation) as any)
    //   .addNode('node1', node1)
    //   .addNode('node2', node2)
    //   .addEdge(START, 'node1')
    //   .addEdge('node1', 'node2')
    //   .addEdge('node2', END)
    //   .compile();

    const node3 = async (state: any) => {
      console.log('Node 3 executed, current messages:');

      const chatPrompt = ChatPromptTemplate.fromMessages([
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'human', content: '{prompt}' },
      ]);

      const aiResponse = await chatPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser())
        .invoke({ prompt: '3 ê³±í•˜ê¸° 3ì€?' });

      return { messages: aiResponse };
    };

    // ëª¨ë“  ë³‘ë ¬ ë…¸ë“œì˜ ê²°ê³¼ë¥¼ ë°›ì€ í›„ ì‹¤í–‰ë˜ëŠ” ë…¸ë“œ
    const node4 = async (state: any) => {
      console.log('Node 4 executed, current messagess:');
      const chatPrompt = ChatPromptTemplate.fromMessages([
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'human', content: '{prompt}' },
      ]);

      const aiResponse = await chatPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser())
        .invoke({ prompt: '3 ê³±í•˜ê¸° 4ëŠ”?' }, { timeout: isErr ? 100 : 5000 });

      return { messages: aiResponse };
    };

    const node5 = (state: any) => {
      console.log('Node 5 executed, current messages:');
      return { messages: 'e' };
    };

    const node6 = (state: any) => {
      console.log('Node 6 executed ONCE, current messages:');
      return { messages: 'f' };
    };

    const workflow2 = (new StateGraph(MessagesAnnotation) as any)
      .addNode('node3', node3)
      .addNode('node4', node4)
      .addNode('node5', node5)
      .addNode('node6', node6)
      .addEdge(START, 'node3')
      .addEdge('node3', 'node4')
      .addEdge('node4', 'node5')
      .addEdge('node5', 'node6')
      .addEdge('node6', END)
      .compile({ checkpointer: this.checkpointer });

    // âœ… ê°„ë‹¨í•œ í•´ê²°ì±…: ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    const workflow = (new StateGraph(MessagesAnnotation) as any)
      .addNode('node1', node1)
      .addNode('node2', node2)
      .addNode('subGraph', workflow2)
      // .addNode('node3', node3)
      // .addNode('node4', node4)
      // .addNode('node5', node5)
      // .addNode('node6', node6)
      .addNode('openai', openAiFunc)
      .addEdge(START, 'node1') // START â†’ wf1 (a,b)
      .addEdge('node1', 'node2') // wf1 â†’ wf2 (c,d)
      .addEdge('node1', 'subGraph') // wf2 â†’ node5 (e)
      .addEdge('node2', 'openai') // wf2 â†’ node5 (e)
      .addEdge('subGraph', 'openai') // wf2 â†’ node5 (e)
      // .addEdge('node4', 'node5') // wf2 â†’ node5 (e)
      // .addEdge('node5', 'node6') // node5 â†’ node6 (f) - ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ í•œ ë²ˆë§Œ!
      // .addEdge('node6', 'openai')
      .addEdge('openai', END);

    const compiledGraph = workflow.compile({ checkpointer: this.checkpointer });

    // âœ… Mermaid ë‹¤ì´ì–´ê·¸ëž¨ ì½˜ì†” ì¶œë ¥
    try {
      const drawableGraph = compiledGraph.getGraph();
      const mermaidCode = await drawableGraph.drawMermaid();
      console.log('ðŸ“Š Graph Diagram (Mermaid):');
      console.log(mermaidCode);
    } catch (error) {
      console.log('âš ï¸ Diagram generation failed:', error);
    }

    // âœ… Thread ID ê¸°ë°˜ ì²´í¬í¬ì¸í„° ì„¤ì •
    const config = {
      configurable: {
        thread_id: `user_${userId}`,
      },
    };

    // ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    const existingCheckpoint = await compiledGraph.getState(config);
    console.log('ðŸ“‹ Existing checkpoint:', existingCheckpoint);

    let result;

    if (
      existingCheckpoint &&
      existingCheckpoint.values &&
      Object.keys(existingCheckpoint.values).length > 0
    ) {
      const a = existingCheckpoint.tasks.find(
        (task) => task.name == 'subGraph',
      );
      console.log(a);
      console.log('ðŸ”„ Resuming from checkpoint...');
      // ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìž¬ê°œ
      result = await compiledGraph.invoke(null, config);
    } else {
      console.log('ðŸ†• Starting new workflow...');
      // ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ì‹œìž‘
      result = await compiledGraph.invoke(
        {
          messages: [], // ë¹ˆ ë°°ì—´ë¡œ ì‹œìž‘
        },
        config,
      );
    }

    // ìµœì¢… ìƒíƒœ ì €ìž¥ í™•ì¸
    const finalState = await compiledGraph.getState(config);
    console.log('ðŸ’¾ Final state saved:', finalState.next);

    console.log(
      'Final result:',
      result.messages.map((m) => m.content),
    );
    return {
      result: result.messages.map((m) => m.content),
    };
  }

  async openai(prompt: string) {
    const chatPrompt = ChatPromptTemplate.fromMessages([
      { role: 'system', content: 'You are a helpful assistant.' },
      { role: 'human', content: '{prompt}' },
    ]);

    return chatPrompt
      .pipe(this.model)
      .pipe(new StringOutputParser())
      .invoke({ prompt });
  }
}
